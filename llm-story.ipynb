{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T16:05:07.362870Z",
     "iopub.status.busy": "2025-04-19T16:05:07.362710Z",
     "iopub.status.idle": "2025-04-19T16:05:12.807523Z",
     "shell.execute_reply": "2025-04-19T16:05:12.806771Z",
     "shell.execute_reply.started": "2025-04-19T16:05:07.362855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:07:10.300983Z",
     "iopub.status.busy": "2025-04-19T16:07:10.300275Z",
     "iopub.status.idle": "2025-04-19T16:07:10.335531Z",
     "shell.execute_reply": "2025-04-19T16:07:10.334693Z",
     "shell.execute_reply.started": "2025-04-19T16:07:10.300959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Positional Encoding (unchanged but with explicit initialization for clarity)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Enhanced Transformer Model (modest changes)\n",
    "class TransformerStoryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, nhead=8, num_encoder_layers=7, num_decoder_layers=7, dim_feedforward=1536, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'attn': nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                'norm2': nn.LayerNorm(d_model),\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(d_model, dim_feedforward),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(dim_feedforward, d_model),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            }) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'self_attn': nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                'norm2': nn.LayerNorm(d_model),\n",
    "                'cross_attn': nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                'norm3': nn.LayerNorm(d_model),\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(d_model, dim_feedforward),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(dim_feedforward, d_model),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            }) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.norm_out = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[1]\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool, device=src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        src_padding_mask = (src == 0)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n",
    "\n",
    "        # Embeddings\n",
    "        src_emb = self.token_embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.token_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "        src_emb = self.embedding_dropout(src_emb)\n",
    "        tgt_emb = self.embedding_dropout(tgt_emb)\n",
    "\n",
    "        # Encoder\n",
    "        memory = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer['norm1'](memory)\n",
    "            memory = memory + layer['attn'](memory, memory, memory, attn_mask=src_mask, key_padding_mask=src_padding_mask)[0]\n",
    "            memory = layer['norm2'](memory)\n",
    "            memory = memory + layer['ffn'](memory)\n",
    "\n",
    "        # Decoder\n",
    "        output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer['norm1'](output)\n",
    "            output = output + layer['self_attn'](output, output, output, attn_mask=tgt_mask, key_padding_mask=tgt_padding_mask)[0]\n",
    "            output = layer['norm2'](output)\n",
    "            output = output + layer['cross_attn'](output, memory, memory, key_padding_mask=src_padding_mask)[0]\n",
    "            output = layer['norm3'](output)\n",
    "            output = output + layer['ffn'](output)\n",
    "\n",
    "        output = self.norm_out(output)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def encode(self, src):\n",
    "        src_emb = self.token_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        src_emb = self.embedding_dropout(src_emb)\n",
    "        src_padding_mask = (src == 0)\n",
    "        memory = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer['norm1'](memory)\n",
    "            memory = memory + layer['attn'](memory, memory, memory, key_padding_mask=src_padding_mask)[0]\n",
    "            memory = layer['norm2'](memory)\n",
    "            memory = memory + layer['ffn'](memory)\n",
    "        return memory\n",
    "\n",
    "    def decode(self, tgt, memory):\n",
    "        tgt_emb = self.token_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "        tgt_emb = self.embedding_dropout(tgt_emb)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer['norm1'](output)\n",
    "            output = output + layer['self_attn'](output, output, output, attn_mask=tgt_mask, key_padding_mask=tgt_padding_mask)[0]\n",
    "            output = layer['norm2'](output)\n",
    "            output = output + layer['cross_attn'](output, memory, memory)[0]\n",
    "            output = layer['norm3'](output)\n",
    "            output = output + layer['ffn'](output)\n",
    "        output = self.norm_out(output)\n",
    "        return self.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:58:13.278603Z",
     "iopub.status.busy": "2025-04-19T16:58:13.277970Z",
     "iopub.status.idle": "2025-04-19T17:46:21.040605Z",
     "shell.execute_reply": "2025-04-19T17:46:21.039817Z",
     "shell.execute_reply.started": "2025-04-19T16:58:13.278576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading TinyStories dataset...\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load TinyStories dataset\n",
    "print(\"Loading TinyStories dataset...\")\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleTokenizer(tokenization_type='word')\n",
    "\n",
    "# Fit tokenizer on a larger subset of the data\n",
    "print(\"Fitting tokenizer on dataset...\")\n",
    "sample_size = 100000  # Use a larger sample for robust vocabulary\n",
    "sample_texts = [ds[\"train\"][i][\"text\"] for i in range(min(sample_size, len(ds[\"train\"]))) if i < len(ds[\"train\"])]\n",
    "tokenizer.fit(sample_texts)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "seq_length = 128\n",
    "# Create a single dataset from a larger subset of training data\n",
    "max_samples = 100000  # Use 50,000 stories to ensure enough sequences\n",
    "stories = [ds[\"train\"][i][\"text\"] for i in range(min(max_samples, len(ds[\"train\"])))]\n",
    "dataset = StoryDataset(stories, tokenizer, seq_length=seq_length)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Calculate required batch size for ~1000 steps\n",
    "target_steps = 1000\n",
    "estimated_dataset_size = len(train_dataset)\n",
    "batch_size = max(1, estimated_dataset_size // target_steps)  # Ensure at least 1\n",
    "if batch_size > 32:  # Cap batch size to avoid memory issues\n",
    "    batch_size = 32\n",
    "print(f\"Adjusted batch_size: {batch_size}, Estimated steps: {len(train_dataset) // batch_size}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "# Create model\n",
    "model = TransformerStoryModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,      # Embedding dimension\n",
    "    nhead=8,          # Number of attention heads\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=1024\n",
    ").to(device)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader) * 10)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "output_dir = \"/kaggle/working/cusLLM_model\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"tokenizer\": {\n",
    "        \"word_to_idx\": tokenizer.word_to_idx,\n",
    "        \"idx_to_word\": tokenizer.idx_to_word,\n",
    "        \"tokenization_type\": tokenizer.tokenization_type\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"d_model\": model.d_model,\n",
    "        \"vocab_size\": model.vocab_size,\n",
    "    }\n",
    "}, final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:12:17.120291Z",
     "iopub.status.busy": "2025-04-19T18:12:17.119979Z",
     "iopub.status.idle": "2025-04-19T18:13:19.284019Z",
     "shell.execute_reply": "2025-04-19T18:13:19.283458Z",
     "shell.execute_reply.started": "2025-04-19T18:12:17.120271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1245307404.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(final_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_state_dict', 'tokenizer', 'config']\n",
      "word\n",
      "Loaded model from /kaggle/working/cusLLM_model/final_model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt for text generation:  jim said\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "said and it of She little and a break. and bring of She forgot the and find came lived the and to had Lucy off my smiled give time star looked and a her excited the came named and smiled was a her little would and day, the but there and said and the a bird go her the and and lived play Her can hungry. and he and kept and flowers told their and he excited was of rolling the very to her a The play it happy The me the the was He a asked. tray other and and She soon. a the It's They a a on the It Lily made to to and took his she of showed it games the little was on loaded Tommy garden a then toy owl the said He He Anna was you, and a the that to the table. and Billy her needed the was it and not, friends. she and bird to the to was to He big tell saw and loved the bridge happy to and his be \"You for very play finished, hot.\" The that was the and is a The for play. Ben a I'm the soon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next? (y/n): n\n"
     ]
    }
   ],
   "source": [
    "# Define the correct path\n",
    "final_model_path = \"/kaggle/working/cusLLM_model/final_model.pt\"\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(final_model_path, map_location=device)\n",
    "print(list(checkpoint.keys()))\n",
    "print(checkpoint[\"tokenizer\"][\"tokenization_type\"])\n",
    "# Rebuild the tokenizer from saved state\n",
    "tokenizer = SimpleTokenizer(tokenization_type=checkpoint[\"tokenizer\"][\"tokenization_type\"])\n",
    "tokenizer.word_to_idx = checkpoint[\"tokenizer\"][\"word_to_idx\"]\n",
    "tokenizer.idx_to_word = checkpoint[\"tokenizer\"][\"idx_to_word\"]\n",
    "\n",
    "# Reinitialize the model with saved config\n",
    "model = TransformerStoryModel(\n",
    "    vocab_size=checkpoint[\"config\"][\"vocab_size\"],\n",
    "    d_model=checkpoint[\"config\"][\"d_model\"],\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=1024\n",
    ").to(device)\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(f\"Loaded model from {final_model_path}\")\n",
    "\n",
    "while True:\n",
    "    # Get user input for text generation\n",
    "    prompt = input(\"Enter a prompt for text generation: \")\n",
    "    generated_text = generate_story(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_length=200,\n",
    "        temperature=0.8,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Generated text:\\n{generated_text}\")\n",
    "    choice = input(\"next? (y/n):\")\n",
    "    if choice == \"n\":\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
