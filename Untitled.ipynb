{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cbe5f-3d3d-4525-98f1-b54391fce777",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' (1686918023.py, line 310)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 310\u001b[1;36m\u001b[0m\n\u001b[1;33m    sample_texts = [ds[\"train\"][i][\"text\"] for i in range(min(sample_size, len(ds[\"train\"]))) if i < len(ds[\"train\"]))]\u001b[0m\n\u001b[1;37m                                                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '['\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerStoryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, nhead=8, num_encoder_layers=7, num_decoder_layers=7, dim_feedforward=1536, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'attn': nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                'norm2': nn.LayerNorm(d_model),\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(d_model, dim_feedforward),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(dim_feedforward, d_model),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            }) for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'norm1': nn.LayerNorm(d_model),\n",
    "                'self_attn': nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                'norm2': nn.LayerNorm(d_model),\n",
    "                'cross_attn': nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True),\n",
    "                'norm3': nn.LayerNorm(d_model),\n",
    "                'ffn': nn.Sequential(\n",
    "                    nn.Linear(d_model, dim_feedforward),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(dim_feedforward, d_model),\n",
    "                    nn.Dropout(dropout)\n",
    "                )\n",
    "            }) for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        self.norm_out = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[1]\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool, device=src.device)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        src_padding_mask = (src == 0)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n",
    "\n",
    "        # Embeddings\n",
    "        src_emb = self.token_embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.token_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "        src_emb = self.embedding_dropout(src_emb)\n",
    "        tgt_emb = self.embedding_dropout(tgt_emb)\n",
    "\n",
    "        # Encoder\n",
    "        memory = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer['norm1'](memory)\n",
    "            memory = memory + layer['attn'](memory, memory, memory, attn_mask=src_mask, key_padding_mask=src_padding_mask)[0]\n",
    "            memory = layer['norm2'](memory)\n",
    "            memory = memory + layer['ffn'](memory)\n",
    "\n",
    "        # Decoder\n",
    "        output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer['norm1'](output)\n",
    "            output = output + layer['self_attn'](output, output, output, attn_mask=tgt_mask, key_padding_mask=tgt_padding_mask)[0]\n",
    "            output = layer['norm2'](output)\n",
    "            output = output + layer['cross_attn'](output, memory, memory, key_padding_mask=src_padding_mask)[0]\n",
    "            output = layer['norm3'](output)\n",
    "            output = output + layer['ffn'](output)\n",
    "\n",
    "        output = self.norm_out(output)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def encode(self, src):\n",
    "        src_emb = self.token_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.positional_encoding(src_emb)\n",
    "        src_emb = self.embedding_dropout(src_emb)\n",
    "        src_padding_mask = (src == 0)\n",
    "        memory = src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer['norm1'](memory)\n",
    "            memory = memory + layer['attn'](memory, memory, memory, key_padding_mask=src_padding_mask)[0]\n",
    "            memory = layer['norm2'](memory)\n",
    "            memory = memory + layer['ffn'](memory)\n",
    "        return memory\n",
    "\n",
    "    def decode(self, tgt, memory):\n",
    "        tgt_emb = self.token_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.positional_encoding(tgt_emb)\n",
    "        tgt_emb = self.embedding_dropout(tgt_emb)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        tgt_padding_mask = (tgt == 0)\n",
    "        output = tgt_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer['norm1'](output)\n",
    "            output = output + layer['self_attn'](output, output, output, attn_mask=tgt_mask, key_padding_mask=tgt_padding_mask)[0]\n",
    "            output = layer['norm2'](output)\n",
    "            output = output + layer['cross_attn'](output, memory, memory)[0]\n",
    "            output = layer['norm3'](output)\n",
    "            output = output + layer['ffn'](output)\n",
    "        output = self.norm_out(output)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "# Tokenizer\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, tokenization_type='word'):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.tokenization_type = tokenization_type\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.unk_token = '[UNK]'\n",
    "        self.bos_token = '[BOS]'\n",
    "        self.eos_token = '[EOS]'\n",
    "        self.add_special_tokens()\n",
    "\n",
    "    def add_special_tokens(self):\n",
    "        self.word_to_idx = {self.pad_token: 0, self.unk_token: 1, self.bos_token: 2, self.eos_token: 3}\n",
    "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text.split() if self.tokenization_type == 'word' else list(text)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        vocab = set()\n",
    "        for text in texts:\n",
    "            vocab.update(self.tokenize(text))\n",
    "        for token in sorted(vocab):\n",
    "            if token not in self.word_to_idx:\n",
    "                self.word_to_idx[token] = len(self.word_to_idx)\n",
    "                self.idx_to_word[len(self.idx_to_word)] = token\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        tokens = self.tokenize(text)\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        return [self.word_to_idx.get(token, self.word_to_idx[self.unk_token]) for token in tokens]\n",
    "\n",
    "    def decode(self, ids, skip_special_tokens=True):\n",
    "        tokens = [self.idx_to_word.get(id, self.unk_token) for id in ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]]\n",
    "        return ' '.join(tokens) if self.tokenization_type == 'word' else ''.join(tokens)\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.word_to_idx)\n",
    "\n",
    "# Dataset\n",
    "class StoryDataset(Dataset):\n",
    "    def __init__(self, stories, tokenizer, seq_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenized_stories = []\n",
    "        for story in stories:\n",
    "            tokens = tokenizer.encode(story, add_special_tokens=True)\n",
    "            self.tokenized_stories.append(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_stories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenized_stories[idx]\n",
    "        if len(tokens) <= self.seq_length + 1:\n",
    "            tokens = tokens + [0] * (self.seq_length + 1 - len(tokens))\n",
    "        else:\n",
    "            start = random.randint(0, len(tokens) - self.seq_length - 1)\n",
    "            tokens = tokens[start:start + self.seq_length + 1]\n",
    "        src = torch.tensor(tokens[:-1])\n",
    "        tgt = torch.tensor(tokens[1:])\n",
    "        return src, tgt\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs=10):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for src, tgt in progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            output_flat = output.view(-1, model.vocab_size)\n",
    "            target_flat = tgt.contiguous().view(-1)\n",
    "            loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in progress_bar:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                output = model(src, tgt)\n",
    "                output_flat = output.view(-1, model.vocab_size)\n",
    "                target_flat = tgt.contiguous().view(-1)\n",
    "                loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n",
    "                val_loss += loss.item()\n",
    "                progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation loss: {avg_val_loss:.4f}\")\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Text generation\n",
    "def generate_story(model, tokenizer, prompt, max_length=100, temperature=1.0, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    output_ids = input_ids.copy()\n",
    "    for _ in range(max_length):\n",
    "        curr_input = torch.tensor([output_ids[-min(len(output_ids), model.d_model):]], dtype=torch.long).to(device)\n",
    "        memory = model.encode(curr_input)\n",
    "        tgt_input = torch.tensor([[output_ids[-1]]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt_input, memory)\n",
    "        logits = output[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, 1).item()\n",
    "        output_ids.append(next_token_id)\n",
    "        if next_token_id == tokenizer.word_to_idx[tokenizer.eos_token]:\n",
    "            break\n",
    "    return tokenizer.decode(output_ids)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load TinyStories dataset\n",
    "    print(\"Loading TinyStories dataset...\")\n",
    "    ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "    # Create tokenizer\n",
    "    tokenizer = SimpleTokenizer(tokenization_type='word')\n",
    "\n",
    "    # Fit tokenizer on a larger subset of the data\n",
    "    print(\"Fitting tokenizer on dataset...\")\n",
    "    sample_size = 100000  # Use a larger sample for robust vocabulary\n",
    "    sample_texts = [ds[\"train\"][i][\"text\"] for i in range(min(sample_size, len(ds[\"train\"]))) if i < len(ds[\"train\"])]\n",
    "    tokenizer.fit(sample_texts)\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size()\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    seq_length = 128\n",
    "    # Create a single dataset from a larger subset of training data\n",
    "    max_samples = 100000  # Use 100,000 stories to ensure enough sequences\n",
    "    stories = [ds[\"train\"][i][\"text\"] for i in range(min(max_samples, len(ds[\"train\"])))]\n",
    "    dataset = StoryDataset(stories, tokenizer, seq_length=seq_length)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "    val_size = len(dataset) - train_size  # 20% for validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Calculate required batch size for ~1000 steps\n",
    "    target_steps = 1000\n",
    "    estimated_dataset_size = len(train_dataset)\n",
    "    batch_size = max(1, estimated_dataset_size // target_steps)  # Ensure at least 1\n",
    "    if batch_size > 16:  # Cap batch size to avoid memory issues with larger model\n",
    "        batch_size = 16\n",
    "    print(f\"Adjusted batch_size: {batch_size}, Estimated steps: {len(train_dataset) // batch_size}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "    # Create model\n",
    "    model = TransformerStoryModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=384,      # Increased from 256\n",
    "        nhead=8,          # Unchanged\n",
    "        num_encoder_layers=7,  # Increased from 6\n",
    "        num_decoder_layers=7,  # Increased from 6\n",
    "        dim_feedforward=1536   # Increased from 1024\n",
    "    ).to(device)\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)  # Reduced from 0.01 for stability\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader) * 10)\n",
    "\n",
    "    # Train model\n",
    "    trained_model = train_model(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    # Save the final model\n",
    "    output_dir = \"c:/Users/saiet/JupyterNotebooks/tinystories_model/cusLLM_model\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    final_model_path = os.path.join(output_dir, \"final_model.pt\")\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"tokenizer\": {\n",
    "            \"word_to_idx\": tokenizer.word_to_idx,\n",
    "            \"idx_to_word\": tokenizer.idx_to_word,\n",
    "            \"tokenization_type\": tokenizer.tokenization_type\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"d_model\": model.d_model,\n",
    "            \"vocab_size\": model.vocab_size,\n",
    "        }\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "    # Load and test the model\n",
    "    checkpoint = torch.load(final_model_path, map_location=device)\n",
    "    print(list(checkpoint.keys()))\n",
    "    print(checkpoint[\"tokenizer\"][\"tokenization_type\"])\n",
    "    tokenizer = SimpleTokenizer(tokenization_type=checkpoint[\"tokenizer\"][\"tokenization_type\"])\n",
    "    tokenizer.word_to_idx = checkpoint[\"tokenizer\"][\"word_to_idx\"]\n",
    "    tokenizer.idx_to_word = checkpoint[\"tokenizer\"][\"idx_to_word\"]\n",
    "\n",
    "    model = TransformerStoryModel(\n",
    "        vocab_size=checkpoint[\"config\"][\"vocab_size\"],\n",
    "        d_model=checkpoint[\"config\"][\"d_model\"],\n",
    "        nhead=8,\n",
    "        num_encoder_layers=7,\n",
    "        num_decoder_layers=7,\n",
    "        dim_feedforward=1536\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Loaded model from {final_model_path}\")\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt for text generation: \")\n",
    "        generated_text = generate_story(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            temperature=0.8,\n",
    "            device=device\n",
    "        )\n",
    "        print(f\"Generated text:\\n{generated_text}\")\n",
    "        choice = input(\"next? (y/n):\")\n",
    "        if choice == \"n\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601ab2af-9a7f-477f-b24f-1d002da4dcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
