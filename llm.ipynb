{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nimport os\nfrom datasets import load_dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:35:33.907131Z","iopub.execute_input":"2025-04-01T18:35:33.907420Z","iopub.status.idle":"2025-04-01T18:35:40.140369Z","shell.execute_reply.started":"2025-04-01T18:35:33.907382Z","shell.execute_reply":"2025-04-01T18:35:40.139387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Transformer implementation\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length=5000):\n        super().__init__()\n        \n        # Create positional encoding matrix\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        # Register as buffer (not a parameter but part of the module)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass CustomTransformerModel(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=8, num_encoder_layers=6, \n                 num_decoder_layers=6, dim_feedforward=1024, dropout=0.1):\n        super().__init__()\n        \n        # Embedding layers\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model)\n        \n        # Transformer architecture\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n                                                  dim_feedforward=dim_feedforward, \n                                                  dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n        \n        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n                                                  dim_feedforward=dim_feedforward,\n                                                  dropout=dropout, batch_first=True)\n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n        \n        # Output layer\n        self.output_layer = nn.Linear(d_model, vocab_size)\n        \n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        \n    def create_mask(self, src, tgt):\n        src_seq_len = src.shape[1]\n        tgt_seq_len = tgt.shape[1]\n        \n        # Create masks\n        src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool, device=src.device)\n        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n        \n        # Create padding masks\n        src_padding_mask = (src == 0)\n        tgt_padding_mask = (tgt == 0)\n        \n        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n    \n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n    \n    def forward(self, src, tgt):\n        # Embedding and positional encoding\n        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n        \n        # Create masks\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n        \n        # Transformer encoding and decoding\n        memory = self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n        output = self.transformer_decoder(tgt_emb, memory, \n                                         tgt_mask=tgt_mask,\n                                         tgt_key_padding_mask=tgt_padding_mask,\n                                         memory_key_padding_mask=src_padding_mask)\n        \n        # Project to vocabulary size\n        return self.output_layer(output)\n    \n    def encode(self, src):\n        src_emb = self.positional_encoding(self.token_embedding(src) * math.sqrt(self.d_model))\n        src_padding_mask = (src == 0)\n        return self.transformer_encoder(src_emb, src_key_padding_mask=src_padding_mask)\n    \n    def decode(self, tgt, memory):\n        tgt_emb = self.positional_encoding(self.token_embedding(tgt) * math.sqrt(self.d_model))\n        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n        tgt_padding_mask = (tgt == 0)\n        \n        output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n        return self.output_layer(output)\n\n# Custom tokenizer\nclass SimpleTokenizer:\n    def __init__(self, tokenization_type='word'):\n        self.word_to_idx = {}\n        self.idx_to_word = {}\n        self.tokenization_type = tokenization_type  # 'word' or 'char'\n        \n        # Special tokens\n        self.pad_token = '[PAD]'\n        self.unk_token = '[UNK]'\n        self.bos_token = '[BOS]'\n        self.eos_token = '[EOS]'\n        \n        # Add special tokens\n        self.add_special_tokens()\n    \n    def add_special_tokens(self):\n        self.word_to_idx = {\n            self.pad_token: 0,\n            self.unk_token: 1,\n            self.bos_token: 2,\n            self.eos_token: 3\n        }\n        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n    \n    def tokenize(self, text):\n        if self.tokenization_type == 'word':\n            return text.split()\n        else:  # char tokenization\n            return list(text)\n    \n    def fit(self, texts):\n        vocab = set()\n        \n        # Extract all unique tokens\n        for text in texts:\n            tokens = self.tokenize(text)\n            vocab.update(tokens)\n        \n        # Add tokens to vocabulary\n        for token in sorted(vocab):\n            if token not in self.word_to_idx:\n                self.word_to_idx[token] = len(self.word_to_idx)\n                self.idx_to_word[len(self.idx_to_word)] = token\n    \n    def encode(self, text, add_special_tokens=True):\n        tokens = self.tokenize(text)\n        \n        # Add special tokens if needed\n        if add_special_tokens:\n            tokens = [self.bos_token] + tokens + [self.eos_token]\n        \n        # Convert tokens to IDs\n        ids = [self.word_to_idx.get(token, self.word_to_idx[self.unk_token]) for token in tokens]\n        return ids\n    \n    def decode(self, ids, skip_special_tokens=True):\n        tokens = [self.idx_to_word.get(id, self.unk_token) for id in ids]\n        \n        # Remove special tokens if needed\n        if skip_special_tokens:\n            tokens = [token for token in tokens if token not in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]]\n        \n        # Join tokens\n        if self.tokenization_type == 'word':\n            return ' '.join(tokens)\n        else:  # char tokenization\n            return ''.join(tokens)\n    \n    def vocab_size(self):\n        return len(self.word_to_idx)\n\n# Dataset for text generation using TinyStories\nclass TinyStoriesDataset(Dataset):\n    def __init__(self, dataset, tokenizer, seq_length=64, split=\"train\"):\n        self.tokenizer = tokenizer\n        self.seq_length = seq_length\n        self.dataset = dataset[split]\n        \n        # Tokenize the stories\n        print(f\"Tokenizing {split} dataset...\")\n        self.tokenized_stories = []\n        \n        # Process a subset for faster training (adjust as needed)\n        num_samples = min(10000, len(self.dataset))\n        for i in tqdm(range(num_samples)):\n            story = self.dataset[i][\"text\"]\n            tokens = tokenizer.encode(story, add_special_tokens=True)\n            self.tokenized_stories.append(tokens)\n    \n    def __len__(self):\n        return len(self.tokenized_stories)\n    \n    def __getitem__(self, idx):\n        tokens = self.tokenized_stories[idx]\n        \n        # Ensure tokens are the right length\n        if len(tokens) <= self.seq_length + 1:\n            # Pad to sequence length\n            tokens = tokens + [0] * (self.seq_length + 1 - len(tokens))\n        else:\n            # Choose a random starting point to fit sequence length\n            start = random.randint(0, len(tokens) - self.seq_length - 1)\n            tokens = tokens[start:start + self.seq_length + 1]\n        \n        src = torch.tensor(tokens[:-1])\n        tgt = torch.tensor(tokens[1:])\n        \n        return src, tgt\n\n# Training function\ndef train_custom_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs=10):\n    model.train()\n    best_val_loss = float('inf')\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        epoch_loss = 0\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n        \n        for batch_idx, (src, tgt) in enumerate(progress_bar):\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            output = model(src, src)  # Using teacher forcing\n            \n            # Reshape output and target for loss calculation\n            output_flat = output.view(-1, model.vocab_size)\n            target_flat = tgt.contiguous().view(-1)\n            \n            # Calculate loss\n            loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n            \n            # Backward pass and optimize\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            # Update loss\n            epoch_loss += loss.item()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\"loss\": loss.item()})\n        \n        avg_train_loss = epoch_loss / len(train_dataloader)\n        print(f\"Epoch {epoch+1}/{epochs}, Training loss: {avg_train_loss:.4f}\")\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n        \n        with torch.no_grad():\n            for batch_idx, (src, tgt) in enumerate(progress_bar):\n                src, tgt = src.to(device), tgt.to(device)\n                \n                # Forward pass\n                output = model(src, src)\n                \n                # Calculate loss\n                output_flat = output.view(-1, model.vocab_size)\n                target_flat = tgt.contiguous().view(-1)\n                loss = F.cross_entropy(output_flat, target_flat, ignore_index=0)\n                \n                # Update loss\n                val_loss += loss.item()\n                \n                # Update progress bar\n                progress_bar.set_postfix({\"loss\": loss.item()})\n        \n        avg_val_loss = val_loss / len(val_dataloader)\n        print(f\"Epoch {epoch+1}/{epochs}, Validation loss: {avg_val_loss:.4f}\")\n        \n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state_dict\": model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"val_loss\": best_val_loss,\n            }, \"best_model.pt\")\n            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n    \n    return model\n\n# Text generation function\ndef generate_text(model, tokenizer, prompt, max_length=100, temperature=1.0, device=\"cuda\"):\n    model.eval()\n    \n    # Encode prompt\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n    \n    # Initialize output sequence\n    output_ids = input_ids.copy()\n    \n    # Generate one token at a time\n    for _ in range(max_length):\n        # Prepare input (truncate if too long)\n        curr_input = torch.tensor([output_ids[-min(len(output_ids), model.d_model):]], dtype=torch.long).to(device)\n        \n        # Generate memory from encoder\n        memory = model.encode(curr_input)\n        \n        # Create target input (last token)\n        tgt_input = torch.tensor([[output_ids[-1]]], dtype=torch.long).to(device)\n        \n        # Get prediction\n        with torch.no_grad():\n            output = model.decode(tgt_input, memory)\n            \n        # Apply temperature and get probabilities\n        logits = output[0, -1, :] / temperature\n        probs = F.softmax(logits, dim=-1)\n        \n        # Sample from the distribution\n        next_token_id = torch.multinomial(probs, 1).item()\n        \n        # Add to output sequence\n        output_ids.append(next_token_id)\n        \n        # Stop if end of sequence\n        if next_token_id == tokenizer.word_to_idx[tokenizer.eos_token]:\n            break\n    \n    # Decode output sequence\n    generated_text = tokenizer.decode(output_ids)\n    return generated_text\n\n# Main function\ndef main():\n    # Check for GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Load TinyStories dataset\n    print(\"Loading TinyStories dataset...\")\n    ds = load_dataset(\"roneneldan/TinyStories\")\n    \n    # Create tokenizer\n    tokenizer = SimpleTokenizer(tokenization_type='word')\n    \n    # Fit tokenizer on a subset of the data\n    print(\"Fitting tokenizer on dataset...\")\n    sample_size = 2119719  # Adjust based on your needs\n    sample_texts = [ds[\"train\"][i][\"text\"] for i in range(sample_size)]\n    tokenizer.fit(sample_texts)\n    \n    vocab_size = tokenizer.vocab_size()\n    print(f\"Vocabulary size: {vocab_size}\")\n    \n    # Create datasets and dataloaders\n    seq_length = 128\n    train_dataset = TinyStoriesDataset(ds, tokenizer, seq_length=seq_length, split=\"train\")\n    val_dataset = TinyStoriesDataset(ds, tokenizer, seq_length=seq_length, split=\"validation\")\n    \n    batch_size = 32  # Adjust based on your GPU memory\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n    \n    # Create model\n    model = CustomTransformerModel(\n        vocab_size=vocab_size,\n        d_model=256,      # Embedding dimension\n        nhead=8,          # Number of attention heads\n        num_encoder_layers=4,\n        num_decoder_layers=4,\n        dim_feedforward=1024\n    ).to(device)\n    \n    # Setup optimizer and scheduler\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_dataloader) * 10)\n    \n    # Train model\n    trained_model = train_custom_model(\n        model, \n        train_dataloader,\n        val_dataloader,\n        optimizer,\n        scheduler,\n        device,\n        epochs=10 # Adjust based on your needs\n    )\n    \n    # Load best model\n    checkpoint = torch.load(\"best_model.pt\")\n    if checkpoint:\n        print(\"found\")\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    \n    # Generate text\n    prompt = \"Once upon a time\"\n    generated_text = generate_text(\n        model,\n        tokenizer,\n        prompt,\n        max_length=200,\n        temperature=0.8,\n        device=device\n    )\n    \n    print(f\"Generated text:\\n{generated_text}\")\n    \n    # Save final model\n    output_dir = \"/kaggle/working/cusLLM_model\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    torch.save({\n        \"model_state_dict\": model.state_dict(),\n        \"tokenizer\": {\n            \"word_to_idx\": tokenizer.word_to_idx,\n            \"idx_to_word\": tokenizer.idx_to_word,\n            \"tokenization_type\": tokenizer.tokenization_type\n        },\n        \"config\": {\n            \"d_model\": model.d_model,\n            \"vocab_size\": model.vocab_size,\n        }\n    }, \"/kaggle/working/model.pt\")\n    print(f\"Model saved to {output_dir}/model.pt\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T20:25:04.044855Z","iopub.execute_input":"2025-04-01T20:25:04.045189Z","iopub.status.idle":"2025-04-01T20:25:04.192343Z","shell.execute_reply.started":"2025-04-01T20:25:04.045160Z","shell.execute_reply":"2025-04-01T20:25:04.191137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}